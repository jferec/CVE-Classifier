from parsing import parse, bag_of_words
import numpy as np
from main import neural
import json
import time


class Training:
    # Klasy urzadzen
    classes = []
    # Zdania treningowe z klasami
    documents = []
    # Wektory binarne danych treningowych
    vectors = []
    # Macierz wystapien klas w danych treningowych (1 jesli wystepuje, 0 jesli nie)
    output = []

    synapse_0 = 0
    synapse_1 = 0

    def __init__(self, file_name):
        """
        Znajduje i zapamietuje wystepujace w nim klasy.
        Zaklasyfikowane linie parsuje, zapamietuje i wypisuje do pliku, pozostale linie usuwa.
        Tworzy sÅ‚ownik i wektory binarne na podstawie danych treningowych.
        :param file_name: nazwa treningowego pliku
        """
        with open('../text_files/' + file_name, 'r') as input_file:
            input_data = input_file.read()
            lines = input_data.splitlines()

        with open('../text_files/' + 'dictionary.txt', 'w') as output_file:
            for line in lines:
                if ';' in line:
                    self.classes.append(line[-1])
                    line_length = len(line)
                    line = line[:line_length - 3]
                    line = parse.parse_line(line)
                    output_file.write(line + "\n")
                    self.documents.append({"class": self.classes[-1], "sentence": line})

        self.classes = list(set(self.classes))
        self.bag = bag_of_words.BagOfWords.init_from_file("dictionary.txt")
        self.vectors = self.bag.get_binary_vectors("dictionary.txt")

        output_empty = [0] * len(self.classes)
        for doc in self.documents:
            output_row = list(output_empty)
            output_row[self.classes.index(doc['class'])] = 1
            self.output.append(output_row)

        print("Classes: ", len(self.classes))
        print("Documents: ", len(self.documents))

    def train(self, x, y, hidden_neurons=10, alpha=1.0, epochs=50000, dropout=False, dropout_percent=0.5):
        """
        Dostraja parametry funkcji sigmoidalnej i oblicza synapsy, kategoryzujac dane treningowe.
        :param x: macierz wektorow normalnych
        :param y: macierz wystapien klas w danych treningowych
        :param hidden_neurons: ilosc neuronow w warstwie ukrytej
        :param alpha: iloczyn wag w iteracjach
        :param epochs: maksymalna liczba iteracji
        :param dropout: ?
        :param dropout_percent: ?
        :return: zapisuje synapsy do plikow
        """
        start_time = time.time()

        print("Training with %s neurons, alpha:%s, dropout:%s %s" % (
            hidden_neurons, str(alpha), dropout, dropout_percent if dropout else ''))
        print("Input matrix: %sx%s    Output matrix: %sx%s" % (len(x), len(x[0]), 1, len(self.classes)))
        np.random.seed(1)

        last_mean_error = 1
        # Losowo zainicjuj wagi ze srednia 0
        self.synapse_0 = 2 * np.random.random((len(x[0]), hidden_neurons)) - 1
        self.synapse_1 = 2 * np.random.random((hidden_neurons, len(self.classes))) - 1

        prev_synapse_0_weight_update = np.zeros_like(self.synapse_0)
        prev_synapse_1_weight_update = np.zeros_like(self.synapse_1)

        synapse_0_direction_count = np.zeros_like(self.synapse_0)
        synapse_1_direction_count = np.zeros_like(self.synapse_1)

        for j in iter(range(epochs + 1)):
            layer_0 = x
            layer_1 = neural.sigmoid(np.dot(layer_0, self.synapse_0))

            if dropout:
                layer_1 *= np.random.binomial([np.ones((len(x), hidden_neurons))], 1 - dropout_percent)[0] * (
                        1.0 / (1 - dropout_percent))

            layer_2 = neural.sigmoid(np.dot(layer_1, self.synapse_1))

            layer_2_error = y - layer_2

            if (j % 10000) == 0 and j > 5000:
                # Jesli blad jest wiekszy niz poprzednio, przerwij petle
                if np.mean(np.abs(layer_2_error)) < last_mean_error:
                    print("delta after " + str(j) + " iterations:" + str(np.mean(np.abs(layer_2_error))))
                    last_mean_error = np.mean(np.abs(layer_2_error))
                else:
                    print("break:", np.mean(np.abs(layer_2_error)), ">", last_mean_error)
                    break

            # Oblicza w jakim kierunku ida zmiany w l2
            layer_2_delta = layer_2_error * neural.sigmoid_output_to_derivative(layer_2)

            # Oblicza jaki wplyw na blad l2 mialy wartosci z l1
            layer_1_error = layer_2_delta.dot(self.synapse_1.T)

            # Oblicza w jakim kierunku ida zmiany w l1
            layer_1_delta = layer_1_error * neural.sigmoid_output_to_derivative(layer_1)

            synapse_1_weight_update = (layer_1.T.dot(layer_2_delta))
            synapse_0_weight_update = (layer_0.T.dot(layer_1_delta))

            if j > 0:
                synapse_0_direction_count += np.abs(
                    ((synapse_0_weight_update > 0) + 0) - ((prev_synapse_0_weight_update > 0) + 0))
                synapse_1_direction_count += np.abs(
                    ((synapse_1_weight_update > 0) + 0) - ((prev_synapse_1_weight_update > 0) + 0))

            self.synapse_1 += alpha * synapse_1_weight_update
            self.synapse_0 += alpha * synapse_0_weight_update

            prev_synapse_0_weight_update = synapse_0_weight_update
            prev_synapse_1_weight_update = synapse_1_weight_update

        self.save_knowledge()
        print("Training time: ", time.time() - start_time)

    def save_knowledge(self):
        """
        Zapisuje synapsy, klasy i slownik do pliku
        :return: plik knowledge.json
        """
        knowledge = {'synapse_0': self.synapse_0.tolist(), 'synapse_1': self.synapse_1.tolist(),
                     'dictionary': list(self.bag.word_set), 'classes': list(self.classes)}
        with open('../text_files/' + 'knowledge.json', 'w') as output_file:
            json.dump(knowledge, output_file, indent=4, sort_keys=True)


training = Training("dict2017-5-annotated.txt")

vectors_array = np.array(training.vectors)
output_array = np.array(training.output)

training.train(vectors_array, output_array, 20, 0.1, 100000, False, 0.2)
