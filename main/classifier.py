from parsing import bag_of_words, data_loader
from main import network
import numpy as np
import json
import time


class Classifier:

    # Klasy urzadzen
    classes = []

    def __init__(self, training_data_file, hidden_neurons_in_layers):
        """
         Znajduje i zapamietuje wystepujace w nim klasy.
         Zaklasyfikowane linie parsuje, zapamietuje i wypisuje do pliku, pozostale linie usuwa
         Tworzy s≈Çownik i wektory binarne na podstawie danych treningowych.
         :param training_data_file: nazwa treningowego pliku

        """

        self.training_data, self.test_data, self.classes, self.bag = data_loader.load_training_data(training_data_file)

        self.INPUT_L_SIZE = len(self.bag.word_set)
        self.OUTPUT_L_SIZE = len(self.classes)


        print("Classes: ", len(self.classes))

        print("Trained on {0} samples".format(len(self.training_data)))
        print("Tested on {0} samples".format(len(self.test_data)))


        sizes = [self.INPUT_L_SIZE] + hidden_neurons_in_layers + [self.OUTPUT_L_SIZE]
        self.nnetwork = network.Network(sizes)

    def train(self, epochs, mini_batch_size, alpha, test=False):

        start_time = time.time()
        print("Training with mini_batch_size: %s , alpha: %s," % (
            str(mini_batch_size), str(alpha)))
        if test:
            self.nnetwork.stochastic_gradient_descent(self.training_data, epochs, mini_batch_size, alpha, self.test_data)
        else:
            self.nnetwork.stochastic_gradient_descent(self.training_data, epochs, mini_batch_size, alpha)
        print("Training time: ", time.time() - start_time)

    def classify(self, filename):
        """
            Klasyfikuje dane z pliku zrodlowego
            :param file_name: nazwa pliku zrodlowego
            :return: dla kazdej linii: przydzielona klasa i procent pewnosci ze wynik jest prawidlowy
        """
        classify_data, documents = data_loader.load_data(self.bag, filename)

        results = self.nnetwork.classify(classify_data)

        with open('../text_files/' + filename + "-classified", 'w') as file:
             for x, y in zip(documents, results):
                print(x["sentence"])
                print(self.classes[y[0]] + "  [" + str(y[1]) + "]")
                file.write(x["sentence"] + "\n")
                file.write(self.classes[y[0]] + "  [" + str(y[1]) + "]\n")

    def test(self):
        print("Result {0:.2%}".format(self.nnetwork.evaluate(self.test_data) / len(self.test_data)))


    def save(self, filename):
        """Save the neural network to the file ``filename``."""
        data = {"sizes": self.nnetwork.sizes,
                "weights": [w.tolist() for w in self.nnetwork.weights],
                "biases": [b.tolist() for b in self.nnetwork.biases],
                'dictionary': list(self.bag.word_set),
                'classes': list(self.classes)}

        f = open('../text_files/' + filename, 'w')
        json.dump(data, f, indent=4, sort_keys=True)
        f.close()

    def load(self, filename):
        """
        Laduje z pliku .json synapsy, klasy i slownik
        :param filename: nazwa pliku zrodlowego typu .json
        """
        with open('../text_files/' + filename, 'r') as file:
            data = json.load(file)
            weights = [np.array(w) for w in data["weights"]]
            biases = [np.array(b) for b in data["biases"]]
            self.classes = np.asarray(data['classes'])
            print("\nClasses: ", self.classes)
            dictionary = np.asarray(data['dictionary'])
            self.bag = bag_of_words.BagOfWords(dictionary)
            sizes = data["sizes"]

            self.nnetwork = network.Network(sizes)
            self.nnetwork.weights = weights
            self.nnetwork.biases = biases



classifier = Classifier("dict2017-5-annotated.txt", [15])
classifier.train(100, 20, 1.0, True)

# classifier.classify("dict2015-5.txt")

# classifier.save("result.json")
#
# classifier.load("result.json")
# classifier.test()