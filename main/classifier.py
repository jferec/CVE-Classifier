from parsing import bag_of_words, data_loader
from main import network
import numpy as np
import json
import time


class Classifier:

    # Klasy urzadzen
    classes = []

    def __init__(self, training_data_file, hidden_neurons_in_layers):
        """
         Znajduje i zapamietuje wystepujace w nim klasy.
         Zaklasyfikowane linie parsuje, zapamietuje i wypisuje do pliku, pozostale linie usuwa
         Tworzy s≈Çownik i wektory binarne na podstawie danych treningowych.
         :param training_data_file: nazwa treningowego pliku

        """

        self.training_data, self.test_data, self.classes, self.bag = data_loader.load_data(training_data_file)

        self.INPUT_L_SIZE = len(self.bag.word_set)
        self.OUTPUT_L_SIZE = len(self.classes)


        print("Classes: ", len(self.classes))

        print("Trained on {0} samples".format(len(self.training_data)))
        print("Tested on {0} samples".format(len(self.test_data)))


        sizes = [self.INPUT_L_SIZE] + hidden_neurons_in_layers + [self.OUTPUT_L_SIZE]
        self.nnetwork = network.Network(sizes)

    def train(self, epochs, mini_batch_size, alpha, test=False):

        start_time = time.time()
        print("Training with mini_batch_size: %s , alpha: %s," % (
            str(mini_batch_size), str(alpha)))
        if test:
            self.nnetwork.stochastic_gradient_descent(self.training_data, epochs, mini_batch_size, alpha, self.test_data)
        else:
            self.nnetwork.stochastic_gradient_descent(self.training_data, epochs, mini_batch_size, alpha)
        print("Training time: ", time.time() - start_time)

    def classify(self, filename):
        """
            Klasyfikuje dane z pliku zrodlowego
            :param file_name: nazwa pliku zrodlowego
            :return: dla kazdej linii: przydzielona klasa i procent pewnosci ze wynik jest prawidlowy
        """

    def save(self, filename):
        """Save the neural network to the file ``filename``."""
        data = {"sizes": self.nnetwork.sizes,
                "weights": [w.tolist() for w in self.nnetwork.weights],
                "biases": [b.tolist() for b in self.nnetwork.biases],
                'dictionary': list(self.bag.word_set),
                'classes': list(self.classes)}

        f = open(filename, "w")
        json.dump(data, f, indent=4, sort_keys=True)
        f.close()

    def load(self, filename):
        """
        Laduje z pliku .json synapsy, klasy i slownik
        :param filename: nazwa pliku zrodlowego typu .json
        """
        with open(filename, "r") as file:
            data = json.load(file)
            self.nnetwork.weights = [np.array(w) for w in data["weights"]]
            self.nnetwork.biases = [np.array(b) for b in data["biases"]]
            self.classes = np.asarray(data['classes'])
            print("\nClasses: ", self.classes)
            dictionary = np.asarray(data['dictionary'])
            self.bag = bag_of_words.BagOfWords(dictionary)


classifier = Classifier("dict2017-5-annotated.txt", [15])
classifier.train(1000, 20, 1.0, True)