import random
import numpy as np
from parsing import parse
from parsing import bag_of_words

def load_training_data(data_filename):
    """
        Wczytanie danych do treningu z pliku, dane dzieli na treningowe i testowe (4/5 na trening, 1/5 na testy)
        :param data_filename: nazwa pliku
        :return: dane treningowe, dane testowe, slowinik, liste kategorii
    """

    classes = ['C', 'P', 'E', 'H', 'S', 'M']

    with open('../text_files/' + data_filename, 'r') as input_file:
        input_data = input_file.read()
        input_lines = input_data.splitlines()

    lines = []
    for line in input_lines:
        if ';' in line:
            lines.append(line)

    random.shuffle(lines)
    test_lines = lines[:int(len(lines) / 5)]
    training_lines = lines[int(len(lines) / 5):]

    documents = []
    t_documents = []
    bag = bag_of_words.BagOfWords([])

    with open('../text_files/' + 'dictionary.txt', 'w') as output_file:
        for line in training_lines:
            if ';' in line:
                line_class = line[-1]
                line_length = len(line)
                line = line[:line_length - 3]
                line = parse.parse_line(line)
                output_file.write(line + "\n")
                bag.append(line.split())
                documents.append({"class": line_class, "sentence": line})


    for line in test_lines:
        if ';' in line:
            line_class = line[-1]
            line_length = len(line)
            line = line[:line_length - 3]
            line = parse.parse_line(line)
            t_documents.append({"class": line_class, "sentence": line})

    training_input = bag.get_binary_vectors(documents)
    training_output = []


    for doc in documents:
        output_row = np.zeros((len(classes)), dtype=int)
        output_row[classes.index(doc['class'])] = 1
        training_output.append(output_row)

    test_input = bag.get_binary_vectors(t_documents)
    test_output = []

    for doc in t_documents:
        output_row = np.zeros((len(classes)), dtype=int)
        output_row[classes.index(doc['class'])] = 1
        test_output.append(output_row)

    INPUT_L_SIZE = len(bag.word_set)
    OUTPUT_L_SIZE = len(classes)

    tr_input_array = [np.reshape(x, (INPUT_L_SIZE, 1)) for x in training_input]
    tr_output_array = [np.reshape(y, (OUTPUT_L_SIZE, 1)) for y in training_output]

    training_data = list(zip(tr_input_array, tr_output_array))

    input_array = [np.reshape(x, (INPUT_L_SIZE, 1)) for x in test_input]
    output_array = [np.reshape(y, (OUTPUT_L_SIZE, 1)) for y in test_output]

    test_data = list(zip(input_array, output_array))

    return training_data, test_data, classes, bag


def load_data(bag, data_filename):
    """
           Wczytanie danych do klasyfikacji z pliku
           :param data_filename: nazwa pliku
           :return: classify_input - dane do klasyfikacji w postaci wektorow binarnych,
                    documents - sparsowane rekordy
       """
    documents = []

    with open('../text_files/' + data_filename, 'r') as input_file:
        input_data = input_file.read()
        input_lines = input_data.splitlines()

    for line in input_lines:
        line = parse.parse_line(line)
        documents.append({"sentence": line})

    input = bag.get_binary_vectors(documents)
    classify_input = [np.reshape(x, (len(bag.word_set), 1)) for x in input]

    return classify_input, documents